# ClamUI Log Manager Module
"""
Log manager module for ClamUI providing log persistence and retrieval.
Stores scan/update operation logs and provides daemon log access.
"""

import json
import os
import subprocess
import tempfile
import threading
import uuid
from dataclasses import asdict, dataclass, field
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Callable, Optional

from gi.repository import GLib

from .utils import is_flatpak, which_host_command, wrap_host_command


class LogType(Enum):
    """Type of log entry."""
    SCAN = "scan"
    UPDATE = "update"


class DaemonStatus(Enum):
    """Status of the clamd daemon."""
    RUNNING = "running"
    STOPPED = "stopped"
    NOT_INSTALLED = "not_installed"
    UNKNOWN = "unknown"


@dataclass
class LogEntry:
    """A single log entry for a scan or update operation."""
    id: str
    timestamp: str  # ISO format string for JSON serialization
    type: str  # "scan" or "update"
    status: str  # e.g., "clean", "infected", "success", "error"
    summary: str
    details: str
    path: Optional[str] = None  # Scanned path (for scans)
    duration: float = 0.0  # Operation duration in seconds
    scheduled: bool = False  # Whether this was a scheduled automatic scan

    @classmethod
    def create(
        cls,
        log_type: str,
        status: str,
        summary: str,
        details: str,
        path: Optional[str] = None,
        duration: float = 0.0,
        scheduled: bool = False
    ) -> "LogEntry":
        """
        Create a new LogEntry with auto-generated id and timestamp.

        Args:
            log_type: Type of operation ("scan" or "update")
            status: Status of the operation
            summary: Brief description of the operation
            details: Full output/details
            path: Scanned path (for scan operations)
            duration: Operation duration in seconds
            scheduled: Whether this was a scheduled automatic scan

        Returns:
            New LogEntry instance
        """
        return cls(
            id=str(uuid.uuid4()),
            timestamp=datetime.now().isoformat(),
            type=log_type,
            status=status,
            summary=summary,
            details=details,
            path=path,
            duration=duration,
            scheduled=scheduled
        )

    def to_dict(self) -> dict:
        """Convert LogEntry to dictionary for JSON serialization."""
        return asdict(self)

    @classmethod
    def from_dict(cls, data: dict) -> "LogEntry":
        """Create LogEntry from dictionary."""
        return cls(
            id=data.get("id", str(uuid.uuid4())),
            timestamp=data.get("timestamp", datetime.now().isoformat()),
            type=data.get("type", "unknown"),
            status=data.get("status", "unknown"),
            summary=data.get("summary", ""),
            details=data.get("details", ""),
            path=data.get("path"),
            duration=data.get("duration", 0.0),
            scheduled=data.get("scheduled", False)
        )

    @classmethod
    def from_scan_result_data(
        cls,
        scan_status: str,
        path: str,
        duration: float,
        scanned_files: int = 0,
        scanned_dirs: int = 0,
        infected_count: int = 0,
        threat_details: Optional[list] = None,
        error_message: Optional[str] = None,
        stdout: str = "",
        suffix: str = "",
        scheduled: bool = False
    ) -> "LogEntry":
        """
        Create a LogEntry from scan result data.

        This factory method handles the common logic for creating log entries
        from scan results, used by both Scanner and DaemonScanner.

        Args:
            scan_status: Status of the scan ("clean", "infected", "cancelled", "error")
            path: The path that was scanned
            duration: Scan duration in seconds
            scanned_files: Number of files scanned
            scanned_dirs: Number of directories scanned
            infected_count: Number of infections found
            threat_details: List of threat details (dicts with file_path, threat_name)
            error_message: Error message if status is error
            stdout: Raw stdout from scan command
            suffix: Optional suffix for summary (e.g., "(daemon)")
            scheduled: Whether this was a scheduled scan

        Returns:
            New LogEntry instance
        """
        threat_details = threat_details or []

        # Build summary based on status
        suffix_str = f" {suffix}" if suffix else ""
        if scan_status == "clean":
            summary = f"Clean scan of {path}{suffix_str}"
            status = "clean"
        elif scan_status == "infected":
            summary = f"Found {infected_count} threat(s) in {path}{suffix_str}"
            status = "infected"
        elif scan_status == "cancelled":
            summary = f"Scan cancelled: {path}"
            status = "cancelled"
        else:
            summary = f"Scan error: {path}"
            status = "error"

        # Build details string
        details_parts = []
        if scanned_files > 0:
            details_parts.append(f"Scanned: {scanned_files} files, {scanned_dirs} directories")
        if infected_count > 0:
            details_parts.append(f"Threats found: {infected_count}")
            for threat in threat_details:
                file_path = threat.get("file_path", threat.get("path", "unknown"))
                threat_name = threat.get("threat_name", threat.get("name", "unknown"))
                details_parts.append(f"  - {file_path}: {threat_name}")
        if error_message:
            details_parts.append(f"Error: {error_message}")
        details = "\n".join(details_parts) if details_parts else stdout or ""

        return cls.create(
            log_type="scan",
            status=status,
            summary=summary,
            details=details,
            path=path,
            duration=duration,
            scheduled=scheduled
        )


# Common locations for clamd log files
CLAMD_LOG_PATHS = [
    "/var/log/clamav/clamd.log",
    "/var/log/clamd.log",
]

# Index file for optimized log retrieval
INDEX_FILENAME = "log_index.json"


class LogManager:
    """
    Manager for log persistence and retrieval.

    Provides methods for saving scan/update logs, retrieving historical logs,
    and accessing clamd daemon logs.

    Index Schema:
        The log index file (log_index.json) contains metadata for fast log retrieval:
        {
            "version": 1,
            "entries": [
                {"id": "uuid-string", "timestamp": "ISO-8601-string", "type": "scan|update"},
                ...
            ]
        }
    """

    def __init__(self, log_dir: Optional[str] = None):
        """
        Initialize the LogManager.

        Args:
            log_dir: Optional custom log directory. Defaults to XDG_DATA_HOME/clamui/logs
        """
        if log_dir:
            self._log_dir = Path(log_dir)
        else:
            xdg_data_home = os.environ.get("XDG_DATA_HOME", "~/.local/share")
            self._log_dir = Path(xdg_data_home).expanduser() / "clamui" / "logs"

        # Thread lock for safe concurrent access
        self._lock = threading.Lock()

        # Ensure log directory exists
        self._ensure_log_dir()

    def _ensure_log_dir(self) -> None:
        """Ensure the log directory exists."""
        try:
            self._log_dir.mkdir(parents=True, exist_ok=True)
        except (OSError, PermissionError):
            # Handle silently - will fail on write operations
            pass

    @property
    def _index_path(self) -> Path:
        """
        Get the path to the log index file.

        Returns:
            Path object pointing to log_index.json in the log directory
        """
        return self._log_dir / INDEX_FILENAME

    def _load_index(self) -> dict:
        """
        Load the log index from file.

        Returns a dictionary with 'version' and 'entries' keys. If the file doesn't
        exist or is corrupted, returns an empty structure with version 1 and empty entries list.

        Returns:
            Dictionary with structure: {"version": 1, "entries": [...]}
        """
        try:
            if self._index_path.exists():
                with open(self._index_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    # Validate structure has required keys
                    if isinstance(data, dict) and "version" in data and "entries" in data:
                        return data
            # File doesn't exist or invalid structure - return empty index
            return {"version": 1, "entries": []}
        except (OSError, json.JSONDecodeError, PermissionError):
            # Handle file read errors and JSON parsing errors gracefully
            return {"version": 1, "entries": []}

    def _save_index(self, index_data: dict) -> bool:
        """
        Atomically save the log index to file.

        Uses a temporary file and rename pattern to prevent corruption
        during write operations (crash safety).

        Args:
            index_data: Dictionary with structure {"version": 1, "entries": [...]}

        Returns:
            True if saved successfully, False otherwise
        """
        try:
            # Ensure parent directory exists
            self._log_dir.mkdir(parents=True, exist_ok=True)

            # Atomic write using temp file + rename
            fd, temp_path = tempfile.mkstemp(
                suffix=".json",
                prefix="log_index_",
                dir=self._log_dir,
            )
            try:
                with os.fdopen(fd, "w", encoding="utf-8") as f:
                    json.dump(index_data, f, indent=2)

                # Atomic rename
                temp_path_obj = Path(temp_path)
                temp_path_obj.replace(self._index_path)
                return True
            except Exception:
                # Clean up temp file on failure
                try:
                    Path(temp_path).unlink(missing_ok=True)
                except OSError:
                    pass
                raise

        except Exception:
            # Catch all exceptions (including OSError, PermissionError)
            return False

    def save_log(self, entry: LogEntry) -> bool:
        """
        Save a log entry to storage.

        Args:
            entry: The LogEntry to save

        Returns:
            True if saved successfully, False otherwise
        """
        with self._lock:
            try:
                self._ensure_log_dir()
                log_file = self._log_dir / f"{entry.id}.json"
                with open(log_file, "w", encoding="utf-8") as f:
                    json.dump(entry.to_dict(), f, indent=2)
                return True
            except (OSError, PermissionError, json.JSONDecodeError):
                return False

    def get_logs(self, limit: int = 100, log_type: Optional[str] = None) -> list[LogEntry]:
        """
        Retrieve stored log entries, sorted by timestamp (newest first).

        Args:
            limit: Maximum number of entries to return
            log_type: Optional filter by type ("scan" or "update")

        Returns:
            List of LogEntry objects
        """
        entries = []

        with self._lock:
            try:
                if not self._log_dir.exists():
                    return entries

                for log_file in self._log_dir.glob("*.json"):
                    try:
                        with open(log_file, "r", encoding="utf-8") as f:
                            data = json.load(f)
                            entry = LogEntry.from_dict(data)

                            # Apply type filter if specified
                            if log_type is None or entry.type == log_type:
                                entries.append(entry)
                    except (OSError, json.JSONDecodeError):
                        # Skip corrupted files
                        continue

            except OSError:
                return entries

        # Sort by timestamp (newest first) and apply limit
        entries.sort(key=lambda e: e.timestamp, reverse=True)
        return entries[:limit]

    def get_logs_async(
        self,
        callback: Callable[[list["LogEntry"]], None],
        limit: int = 100,
        log_type: Optional[str] = None
    ) -> None:
        """
        Retrieve stored log entries asynchronously.

        The log retrieval runs in a background thread and the callback is invoked
        on the main GTK thread via GLib.idle_add when complete.

        Args:
            callback: Function to call with list of LogEntry objects when loading completes
            limit: Maximum number of entries to return
            log_type: Optional filter by type ("scan" or "update")
        """
        def _load_logs_thread():
            try:
                entries = self.get_logs(limit=limit, log_type=log_type)
            except Exception:
                # On any error, return empty list to ensure callback is always called
                # This prevents loading state from getting stuck forever
                entries = []
            # Schedule callback on main thread - always called to reset loading state
            GLib.idle_add(callback, entries)

        thread = threading.Thread(target=_load_logs_thread)
        thread.daemon = True
        thread.start()

    def get_log_by_id(self, log_id: str) -> Optional[LogEntry]:
        """
        Retrieve a specific log entry by ID.

        Args:
            log_id: The UUID of the log entry

        Returns:
            LogEntry if found, None otherwise
        """
        with self._lock:
            try:
                log_file = self._log_dir / f"{log_id}.json"
                if log_file.exists():
                    with open(log_file, "r", encoding="utf-8") as f:
                        data = json.load(f)
                        return LogEntry.from_dict(data)
            except (OSError, json.JSONDecodeError):
                pass
        return None

    def delete_log(self, log_id: str) -> bool:
        """
        Delete a specific log entry.

        Args:
            log_id: The UUID of the log entry to delete

        Returns:
            True if deleted successfully, False otherwise
        """
        with self._lock:
            try:
                log_file = self._log_dir / f"{log_id}.json"
                if log_file.exists():
                    log_file.unlink()
                    return True
            except OSError:
                pass
        return False

    def clear_logs(self) -> bool:
        """
        Clear all stored log entries.

        Returns:
            True if cleared successfully, False otherwise
        """
        with self._lock:
            try:
                if self._log_dir.exists():
                    for log_file in self._log_dir.glob("*.json"):
                        try:
                            log_file.unlink()
                        except OSError:
                            pass
                return True
            except OSError:
                return False

    def get_log_count(self) -> int:
        """
        Get the total number of stored logs.

        Returns:
            Number of log entries
        """
        with self._lock:
            try:
                if not self._log_dir.exists():
                    return 0
                return len(list(self._log_dir.glob("*.json")))
            except OSError:
                return 0

    def get_daemon_status(self) -> tuple[DaemonStatus, Optional[str]]:
        """
        Check the status of the clamd daemon.

        Returns:
            Tuple of (DaemonStatus, optional_message)
        """
        # Check if clamd is installed (checking host if in Flatpak)
        clamd_path = which_host_command("clamd")
        if clamd_path is None:
            return (DaemonStatus.NOT_INSTALLED, "clamd is not installed")

        # Check if clamd process is running (on host if in Flatpak)
        try:
            result = subprocess.run(
                wrap_host_command(["pgrep", "-x", "clamd"]),
                capture_output=True,
                text=True,
                timeout=5
            )
            if result.returncode == 0:
                return (DaemonStatus.RUNNING, "clamd daemon is running")
            else:
                return (DaemonStatus.STOPPED, "clamd daemon is not running")
        except (subprocess.SubprocessError, FileNotFoundError, OSError):
            return (DaemonStatus.UNKNOWN, "Unable to determine daemon status")

    def _file_exists_on_host(self, path: str) -> bool:
        """
        Check if a file exists, using host filesystem if in Flatpak.

        Args:
            path: Path to check

        Returns:
            True if file exists, False otherwise
        """
        if is_flatpak():
            try:
                result = subprocess.run(
                    ["flatpak-spawn", "--host", "test", "-f", path],
                    capture_output=True,
                    timeout=5
                )
                return result.returncode == 0
            except Exception:
                return False
        return Path(path).exists()

    def get_daemon_log_path(self) -> Optional[str]:
        """
        Find the clamd log file path.

        Checks common locations for the clamd log file.

        Returns:
            Path to the log file if found, None otherwise
        """
        for log_path in CLAMD_LOG_PATHS:
            if self._file_exists_on_host(log_path):
                return log_path

        # Also try to get from clamd.conf if it exists
        clamd_conf_paths = [
            "/etc/clamav/clamd.conf",
            "/etc/clamd.conf",
            "/etc/clamd.d/scan.conf",
        ]

        for conf_path in clamd_conf_paths:
            if self._file_exists_on_host(conf_path):
                try:
                    # Read config file (use host command in Flatpak)
                    if is_flatpak():
                        result = subprocess.run(
                            ["flatpak-spawn", "--host", "cat", conf_path],
                            capture_output=True,
                            text=True,
                            timeout=5
                        )
                        if result.returncode != 0:
                            continue
                        config_content = result.stdout
                    else:
                        with open(conf_path, "r", encoding="utf-8") as f:
                            config_content = f.read()

                    for line in config_content.splitlines():
                        line = line.strip()
                        if line.startswith("LogFile"):
                            parts = line.split(None, 1)
                            if len(parts) == 2:
                                log_file = parts[1].strip()
                                if self._file_exists_on_host(log_file):
                                    return log_file
                except (OSError, PermissionError, subprocess.SubprocessError):
                    continue

        return None

    def read_daemon_logs(self, num_lines: int = 100) -> tuple[bool, str]:
        """
        Read the last N lines from the clamd daemon log.

        Uses tail-like behavior to read only the end of the file,
        avoiding loading large log files into memory.

        Tries multiple methods in order:
        1. tail command (wrapped for Flatpak)
        2. journalctl for systemd-based systems
        3. Direct file read (non-Flatpak only)

        Args:
            num_lines: Number of lines to read from the end of the log

        Returns:
            Tuple of (success, log_content_or_error_message)
        """
        log_path = self.get_daemon_log_path()

        # Try reading log file with tail
        if log_path is not None:
            try:
                # Use tail command - wrapped for Flatpak host access
                tail_cmd = wrap_host_command(
                    ["tail", "-n", str(num_lines), log_path]
                )
                result = subprocess.run(
                    tail_cmd,
                    capture_output=True,
                    text=True,
                    timeout=10
                )

                if result.returncode == 0:
                    content = result.stdout
                    if not content.strip():
                        return (True, "(Log file is empty)")
                    return (True, content)
                # If tail failed (permission denied), fall through to journalctl

            except subprocess.TimeoutExpired:
                return (False, "Timeout reading log file")
            except FileNotFoundError:
                pass  # Fall through to journalctl
            except OSError:
                pass  # Fall through to journalctl

        # Try journalctl as fallback (works on systemd systems, no root needed)
        journalctl_result = self._read_daemon_logs_journalctl(num_lines)
        if journalctl_result[0]:
            return journalctl_result

        # If we found a log path but couldn't read it, give helpful error
        if log_path is not None:
            return (
                False,
                f"Permission denied reading {log_path}\n\n"
                "The daemon log file requires elevated permissions.\n"
                "Options:\n"
                "  • Add your user to the 'adm' or 'clamav' group:\n"
                "    sudo usermod -aG adm $USER\n"
                "  • Or check if clamd logs to systemd journal:\n"
                "    journalctl -u clamav-daemon"
            )

        return (
            False,
            "Daemon log file not found.\n\n"
            "ClamAV daemon (clamd) may not be installed or configured.\n"
            "Common log locations checked:\n"
            "  • /var/log/clamav/clamd.log\n"
            "  • /var/log/clamd.log"
        )

    def _read_daemon_logs_journalctl(
        self, num_lines: int
    ) -> tuple[bool, str]:
        """
        Read daemon logs from systemd journal.

        Args:
            num_lines: Number of lines to read

        Returns:
            Tuple of (success, content_or_error)
        """
        # Try different unit names used by various distros
        unit_names = [
            "clamav-daemon",
            "clamav-daemon.service",
            "clamd",
            "clamd.service",
            "clamd@scan",
            "clamd@scan.service",
        ]

        for unit in unit_names:
            try:
                cmd = wrap_host_command([
                    "journalctl",
                    "-u", unit,
                    "-n", str(num_lines),
                    "--no-pager",
                    "-q"  # Quiet - suppress info messages
                ])
                result = subprocess.run(
                    cmd,
                    capture_output=True,
                    text=True,
                    timeout=10
                )

                if result.returncode == 0 and result.stdout.strip():
                    return (True, result.stdout)

            except (subprocess.SubprocessError, FileNotFoundError, OSError):
                continue

        return (False, "No journal entries found for clamd")

    def _read_file_tail(self, file_path: str, num_lines: int) -> tuple[bool, str]:
        """
        Read the last N lines from a file directly (fallback method).

        Args:
            file_path: Path to the file
            num_lines: Number of lines to read

        Returns:
            Tuple of (success, content_or_error)
        """
        try:
            with open(file_path, "r", encoding="utf-8", errors="replace") as f:
                # For small files, read all and return last N lines
                lines = f.readlines()
                tail_lines = lines[-num_lines:] if len(lines) > num_lines else lines
                content = "".join(tail_lines)
                if not content.strip():
                    return (True, "(Log file is empty)")
                return (True, content)
        except PermissionError:
            return (False, "Permission denied reading log file")
        except OSError as e:
            return (False, f"Error reading log file: {e}")
